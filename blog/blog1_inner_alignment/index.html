<!DOCTYPE html>
<html lang="en">
<head>

  <style>
    /* Global */
    body, div {
        font-family: 'Roboto', sans-serif;
        text-align: justify;
        text-justify: inter-word;
    }
  
    /* Navbar */
    #navbar {
        background-color: #f9f9f9;
    }
  
    /* Content and Header Styles */
    h1, h2 {
        font-weight: 500;
        line-height: 1.4;
        color: rgba(0, 0, 0, 0.65);
        margin-top: 0.5em;
        margin-bottom: 0.5em;
        text-align: left;
    }
  
    /* Content Container */
    .content-container {
        margin: 0 auto;
        max-width: 1200px;
        padding: 0 15px;
    }
  
    /* Dividers */
    .horizontal-divider {
        border-top: 0.4px solid grey;
        max-width: 65%;
        margin: 20px auto;
    }
  
    /* Author Details */
    .author-details {
        display: flex;
        justify-content: space-between;
        max-width: 65%;
        margin: 20px auto;
        background-color: #f9f9f9;
        padding: 10px 20px;
        border-radius: 5px;
        box-shadow: 0px 3px 5px rgba(0, 0, 0, 0.1);
    }
  
    .author-details div {
        transition: background-color 0.3s ease, color 0.3s ease;
    }
  
    .author-details div:hover {
        background-color: #007BFF; 
        color: white;
    }
  
    .author-details div strong {
        display: block;
        font-weight: 700;
        margin-bottom: 8px;
    }
  
    /* Table of Contents */
    .toc {
        position: sticky;
        top: 80px;
        margin-top: 10px;
        border: 1px solid #ddd;
        padding: 15px 20px;
        width: 330px;
        max-height: 70vh;
        background-color: #f9f9f9;
        box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
        border-radius: 8px;
        overflow: auto;
        font-size: 0.8em;
        line-height: 1.7em;
        color: rgba(0, 0, 0, 0.5);
        transition: width 0.3s ease;
    }
  
    .toc h3 {
        font-size: 15px;
        margin-top: 1em;
        margin-bottom: 1em;
        border-bottom: 1px solid #ddd;
        padding-bottom: 10px;
    }
  
    .toc ul {
        list-style-type: none;
        padding: 0 0 0 30px;
        margin-top: 5px;
    }
  
    .toc li {
        margin-bottom: 8px;
        transition: transform 0.2s ease;
    }
  
    .toc li:hover {
        transform: translateX(5px);
    }
  
    .toc a {
        color: #444;
        text-decoration: none;
        padding: 5px 8px;
        border-radius: 4px;
        transition: background-color 0.3s ease, color 0.3s ease;
    }
    
    .toc .main-section {
        font-weight: bold;
        margin-top: 2px;
    }
  
    .toc a:hover {
        background-color: #007BFF;
        color: white;
    }
  
    /* Flex Container */
    .flex-container {
        display: flex;
        align-items: start;
        justify-content: center;
        padding: 20px;
        margin-top: 50px;
    }
  
    .main-content {
        flex-grow: 1;
        max-width: 800px;
        margin-left: 40px;
    }
  
    /* Enhanced Blog Title Style */
    .blog-title {
    font-weight: 700;
    color: #333;
    margin-top: 5px;  /* Reduced margin-top value */
    border-bottom: 2px solid #333;
    display: inline-block;
    margin: 0 0 30px; /* Updated margin value */
    padding-bottom: 10px;
    transition: color 0.3s ease;
    text-shadow: 1px 1px 2px rgba(0, 0, 0, 0.1);
}

  
    .blog-title:hover {
      color: #333; /* Changing title color on hover */
    }
  
    /* Styles specific to mobile screens */
    @media (max-width: 767px) {
        .toc {
            position: static;  /* Resetting position */
            top: unset;  /* Unsetting top value */
            width: 100%;  /* Setting width to full */
            margin-bottom: 20px;  /* Adding a bit of space between ToC and main content */
        }
  
        /* Since we're moving ToC to the top, we might want to adjust the .flex-container as well */
        .flex-container {
            flex-direction: column;
        }
  
        .main-content {
            margin-left: 0;  /* Resetting the margin-left */
        }
        
        /* Adjusting author-details for mobile view */
        .author-details {
            flex-direction: column;  /* Stacking the details vertically */
            align-items: center;     /* Centering the content */
            max-width: 100%;         /* Using full width */
            padding: 10px 0;         /* Adjusting padding */
        }
  
        .author-details div {
            text-align: center;      /* Ensuring all text is centered */
            margin-bottom: 10px;     /* Adding some space between each detail */
        }
  
        .blog-title {
            font-size: 1.5em;
            margin: 15px 0 25px;
            padding-bottom: 8px;
        }
    }
  </style>  

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Salman Rahman</title>
  <meta name="description" content="A beautiful Jekyll theme for academics">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/projects/uncertainty/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Salman</span> Rahman</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
            

          <li class="nav-item ">
            <a class="nav-link" href="/research/">
            Research
              
            </a>
        </li>


        
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">
              Publications
              
            </a>
        </li>
            
            
            <li class="nav-item navbar-active font-weight-bold">
              <a class="nav-link" href="/blog/">
                Blogs
                
                  <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
            
                        <li class="nav-item ">
                  <a class="nav-link" href="/assets/pdf/cv_salman.pdf" target="_blank">
                    CV
                    
                  </a>
              </li>
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>



  <div class="flex-container">
    <!-- Table of Contents -->
    <div class="toc">
        <h3>Table of Contents</h3>
        <ol>
            <li><a href="#intro">Introduction</a>
                <ul>
                    <li><a href="#understanding-llm">Understanding LLM reasoning and why it matters</a></li>
                    <li><a href="#framework">The two-dimensional framework: Regimes and Architectures</a></li>
                </ul>
            </li>
            <li><a href="#understanding-framework">Understanding the Reasoning Framework</a>
                <ul>
                    <li><a href="#reasoning-regimes">Reasoning Regimes: Inference Scaling vs. Learning-to-Reason</a></li>
                    <li><a href="#system-architectures">System Architectures: Standalone to Multi-Agent Systems</a></li>
                    <li><a href="#unified-perspective">The unified input-output perspective</a></li>
                </ul>
            </li>
            <li><a href="#inference-scaling">Inference Scaling Approaches</a>
                <ul>
                    <li><a href="#prompt-engineering">Prompt engineering for better reasoning</a></li>
                    <li><a href="#search-planning">Search and planning methods</a></li>
                    <li><a href="#feedback-refinement">Feedback-based refinement</a></li>
                    <li><a href="#tool-integration">Tool integration</a></li>
                </ul>
            </li>
            <li><a href="#learning-to-reason">Learning-to-Reason Methods</a>
                <ul>
                    <li><a href="#data-collection">High-quality data collection strategies</a></li>
                    <li><a href="#supervised-fine-tuning">Supervised fine-tuning for reasoning</a></li>
                    <li><a href="#reinforcement-learning">Reinforcement learning approaches</a></li>
                    <li><a href="#preference-learning">Preference learning techniques</a></li>
                </ul>
            </li>
            <li><a href="#learning-algorithms">Learning Algorithms Behind Reasoning</a>
                <ul>
                    <li><a href="#sft">Supervised Fine-Tuning (SFT)</a></li>
                    <li><a href="#rl">Reinforcement Learning (GRPO, PPO)</a></li>
                    <li><a href="#dpo">Direct Preference Optimization (DPO)</a></li>
                    <li><a href="#training-verifiers">Training verifiers and reward models</a></li>
                </ul>
            </li>
            <li><a href="#domain-specific">Domain-Specific Reasoning Applications</a>
                <ul>
                    <li><a href="#mathematical">Mathematical reasoning</a></li>
                    <li><a href="#code-generation">Code generation</a></li>
                    <li><a href="#tabular-data">Tabular data reasoning</a></li>
                    <li><a href="#game-theoretic">Game-theoretic reasoning</a></li>
                </ul>
            </li>
            <li><a href="#key-trends">Key Trends and Breakthroughs</a>
                <ul>
                    <li><a href="#shift">The shift from inference scaling to learning</a></li>
                    <li><a href="#agentic-systems">The emergence of agentic reasoning systems</a></li>
                    <li><a href="#milestones">Recent model milestones (DeepSeek-R1, OpenAI o1)</a></li>
                </ul>
            </li>
            <li><a href="#open-challenges">Open Challenges and Future Directions</a>
                <ul>
                    <li><a href="#evaluating">Evaluating reasoning beyond outcomes</a></li>
                    <li><a href="#understanding-mechanisms">Understanding reasoning mechanisms</a></li>
                    <li><a href="#data-challenges">Data challenges in scaling reasoning capabilities</a></li>
                    <li><a href="#cost-aware">Cost-aware training approaches</a></li>
                </ul>
            </li>
            <li><a href="#conclusion">Conclusion</a>
                <ul>
                    <li><a href="#future">The future of reasoning in LLMs</a></li>
                    <li><a href="#implications">Implications for AI development</a></li>
                </ul>
            </li>
        </ol>
    </div>

  <!-- Heading and intro -->
  <div class="main-content">

  <div class="content">

    <h1 class="blog-title">From Inference to Learning: The Evolution of LLM Reasoning</h1>
    <p>
      This blog post provides a comprehensive overview of the evolution of reasoning capabilities in Large Language Models (LLMs), exploring the transition from inference-time scaling to learning-based approaches, and examining the implications for future AI development.
    </p>
</div>


<div class="content-container">

  <!-- First Horizontal Divider -->
  <hr class="horizontal-divider">

  <!-- Author's Details -->
  <div class="author-details">
    <div>
      <strong>AUTHORS</strong>
      Salman Rahman
    </div>
    <div>
      <strong>AFFILIATIONS</strong>
      New York University
    </div>
    <div>
      <strong>PUBLISHED</strong>
      May 5, 2024
    </div>
  </div>

  <!-- Second Horizontal Divider -->
  <hr class="horizontal-divider">

</div>







<!-- Blog Content -->

<div class="content">

<!-- Introduction Section -->

  <h2 id="intro" style="font-weight: bold; margin-top: -80px; padding-top: 80px;">Introduction</h2>

  <hr> <!-- Horizontal divider -->

  <p>
      Reasoning—the cognitive process that enables logical inference, problem-solving, and decision-making—has emerged as a critical capability that distinguishes advanced AI systems from conventional models. As large language models (LLMs) continue to advance, their ability to reason has become increasingly important, not just for solving complex problems but as a pathway toward more sophisticated AI systems approaching artificial general intelligence (AGI).
  </p>

  <p>
      But what exactly constitutes reasoning in LLMs? At its core, LLM reasoning involves going beyond direct pattern recognition to generate a step-by-step thinking process in the form of "question → reasoning steps → answer." Rather than simply producing answers from questions, reasoning LLMs can articulate their thought process, making their decision-making more transparent and often more accurate.
  </p>

  <!-- Figure Section -->
  <div class="l-body">
    <figure>
        <img class="img-fluid rounded z-depth-1" src="fig/fig1.png" alt="Description of the figure for accessibility">
        <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;">
            Optional caption for the figure if needed.
        </figcaption>
    </figure>
  </div>

  <h3 id="understanding-llm">Understanding LLM reasoning and why it matters</h3>
  <p>
      The landscape of LLM reasoning research can be organized along two orthogonal dimensions:
  </p>

  <p>
      1. <strong>Reasoning Regimes</strong>: This dimension concerns when and how reasoning is achieved. We can distinguish between:
      - <strong>Inference Scaling</strong>: Enhancing reasoning through additional test-time computation
      - <strong>Learning-to-Reason</strong>: Building reasoning capabilities during the training phase
  </p>

  <p>
      2. <strong>System Architectures</strong>: This dimension addresses what components are involved in the reasoning process:
      - <strong>Standalone LLMs</strong>: Reasoning within a single model
      - <strong>Single-Agent Systems</strong>: LLMs enhanced with external tools or environments
      - <strong>Multi-Agent Systems</strong>: Multiple LLMs interacting to solve problems collaboratively
  </p>

  <h3 id="framework">The two-dimensional framework: Regimes and Architectures</h3>
  <p>
      These dimensions provide a structured framework for understanding the rapidly evolving field of LLM reasoning, highlighting important trends like the shift from inference-time scaling to learning-to-reason approaches, and from standalone models to agentic systems.
  </p>

  <h2 id="understanding-framework" style="font-weight: bold; margin-top: -80px; padding-top: 80px;">Understanding the Reasoning Framework</h2>
  <hr>

  <h3 id="reasoning-regimes">Reasoning Regimes: Inference Scaling vs. Learning-to-Reason</h3>
  <p>
      The first dimension of our framework examines when reasoning capabilities emerge in LLMs:
  </p>

  <p>
      <strong>Inference Scaling</strong> involves techniques that enhance reasoning during test time without updating the model parameters. These methods add computational steps before generating a final answer, allowing the model to "think" before responding. Chain-of-Thought (CoT) prompting is a prime example—by instructing an LLM to "think step by step," we can elicit better reasoning without any fine-tuning.
  </p>

  <p>
      <strong>Learning-to-Reason</strong> shifts the focus to training models specifically for reasoning capabilities. Rather than relying on inference-time computation, this approach incorporates reasoning into the training process itself. Recent work like DeepSeek-R1 demonstrates that models can be trained to reason effectively without extensive inference-time computation.
  </p>

  <h3 id="system-architectures">System Architectures: Standalone to Multi-Agent Systems</h3>
  <p>
      The second dimension explores the components involved in reasoning:
  </p>

  <p>
      <strong>Standalone LLMs</strong> reason entirely within a single model. They process an input prompt and generate outputs that include both answers and the reasoning steps that led to them. While limited to their pre-trained knowledge, these models can still demonstrate impressive reasoning through techniques like self-consistency or multiple sampling.
  </p>

  <p>
      <strong>Single-Agent Systems</strong> enhance standalone LLMs by enabling interaction with external tools or environments. These systems can retrieve information, use specialized tools (like calculators or code interpreters), and receive feedback to refine their reasoning.
  </p>

  <p>
      <strong>Multi-Agent Systems</strong> involve multiple LLMs interacting to solve problems collaboratively. Each agent can take on a specialized role (critic, executor, planner, etc.), allowing for more complex reasoning through the exchange of messages and coordinated actions.
  </p>

  <h3 id="unified-perspective">The unified input-output perspective</h3>
  <p>
      Despite their differences, these approaches can be understood through a unified lens of inputs and outputs. In standalone models, the focus is on constructing better prompts (inputs) and optimizing the generated responses (outputs). In single-agent systems, this translates to perception (input) and action (output). For multi-agent systems, it becomes communication (input) and coordination (output).
  </p>

  <h2 id="inference-scaling" style="font-weight: bold; margin-top: -80px; padding-top: 80px;">Inference Scaling Approaches</h2>
  <hr>

  <h3 id="prompt-engineering">Prompt engineering for better reasoning</h3>
  <p>
      One of the simplest yet most effective ways to improve reasoning in LLMs is through careful prompt design.
  </p>

  <p>
      <strong>Instruction Engineering</strong> focuses on crafting prompts that explicitly elicit reasoning. Templates like "Let's think step by step" have proven remarkably effective. Advanced approaches like Automatic Prompt Engineer (APE) even use LLMs themselves to generate high-quality instructions.
  </p>

  <p>
      <strong>Demonstration Engineering</strong> leverages the power of examples. Analogical prompting guides LLMs to generate relevant examples before solving a problem—for instance, prompting a model to generate a problem about calculating a third-order determinant before solving a fourth-order one.
  </p>

  <h3 id="search-planning">Search and planning methods</h3>
  <p>
      Beyond prompt engineering, inference scaling can employ more sophisticated search and planning methods:
  </p>

  <p>
      <strong>Task Decomposition</strong> breaks complex problems into manageable subtasks. Methods like Least-to-Most Prompting or Plan-and-Solve guide LLMs in approaching problems incrementally, tackling simpler components before addressing the whole.
  </p>

  <p>
      <strong>Exploration and Search</strong> techniques like Tree-of-Thoughts and Graph-of-Thoughts allow LLMs to explore multiple reasoning paths simultaneously, rather than committing to a single chain of thought.
  </p>

  <h3 id="feedback-refinement">Feedback-based refinement</h3>
  <p>
      Feedback mechanisms can significantly enhance reasoning quality during inference:
  </p>

  <p>
      <strong>Verifiers and Reflection</strong> employ either separate models or the same model to evaluate and improve reasoning. Techniques like self-consistency sampling multiple solutions and selecting the most common answer, while more advanced approaches use dedicated verifier models to provide detailed feedback on reasoning steps.
  </p>

  <h3 id="tool-integration">Tool integration</h3>
  <p>
      Integration with external tools expands an LLM's reasoning capabilities:
  </p>

  <p>
      <strong>Retrieval Augmentation</strong> enhances reasoning by enabling access to external knowledge, reducing hallucinations and ensuring more accurate, fact-based responses. This is particularly valuable for knowledge-intensive tasks requiring multi-hop and long-horizon reasoning.
  </p>

  <p>
      <strong>Specialized Tools</strong> like calculators, compilers, or domain-specific APIs help overcome inherent limitations of language models. For example, MATHSENSEI employs tools such as Python, WolframAlpha, and search engines to solve complex mathematical problems with higher accuracy than is possible with an LLM alone.
  </p>

  <h2 id="learning-to-reason" style="font-weight: bold; margin-top: -80px; padding-top: 80px;">Learning-to-Reason Methods</h2>
  <hr>

  <h3 id="data-collection">High-quality data collection strategies</h3>
  <p>
      Effective learning-to-reason requires high-quality training data:
  </p>

  <p>
      <strong>Question Augmentation</strong> expands existing datasets using techniques like rewriting, transforming, or evolving questions to cover a broader range of scenarios and difficulty levels.
  </p>

  <p>
      <strong>Knowledge Graph-Based Synthesis</strong> utilizes structured taxonomies to generate questions with broader knowledge coverage. This approach helps overcome limitations of direct augmentation by ensuring that generated questions span relevant conceptual spaces systematically.
  </p>

  <h3 id="supervised-fine-tuning">Supervised fine-tuning for reasoning</h3>
  <p>
      Supervised Fine-Tuning (SFT) is the most straightforward approach to training reasoning-capable LLMs:
  </p>

  <p>
      <strong>Reasoning Distillation</strong> transfers reasoning capabilities from more powerful models to smaller ones. Projects like the NovaSky Team and Bespoke Labs have successfully distilled reasoning chains from models like OpenAI-o1 and DeepSeek-R1 into smaller, more efficient models.
  </p>

  <h3 id="reinforcement-learning">Reinforcement learning approaches</h3>
  <p>
      Reinforcement Learning (RL) has emerged as a powerful approach for teaching LLMs to reason:
  </p>

  <p>
      <strong>GRPO (Group-Relative Policy Optimization)</strong> samples multiple outputs for each input, computes rewards, and determines the relative advantage of each output within the group. This method has shown remarkable success in DeepSeek-R1, enabling the model to learn sophisticated behaviors like reflection and alternative approach exploration.
  </p>

  <h3 id="preference-learning">Preference learning techniques</h3>
  <p>
      Preference learning optimizes models based on explicit preferences between different reasoning paths:
  </p>

  <p>
      <strong>Direct Preference Optimization (DPO)</strong> and its variants learn directly from preference data without a separate reward model. These approaches use a simple binary classification loss to optimize the policy based on pairs of preferred and non-preferred outputs.
  </p>

  <h2 id="learning-algorithms" style="font-weight: bold; margin-top: -80px; padding-top: 80px;">Learning Algorithms Behind Reasoning</h2>
  <hr>

  <h3 id="sft">Supervised Fine-Tuning (SFT)</h3>
  <p>
      SFT remains the foundation of many reasoning approaches, adapting pre-trained LLMs for reasoning tasks:
  </p>

  <p>
      <strong>Token-Level SFT</strong> optimizes the model to predict the next token given previous tokens, maximizing the log probability of the ground truth sequence. It's the default first step in training reasoning models but may not fully capture the multi-step nature of reasoning.
  </p>

  <h3 id="rl">Reinforcement Learning (GRPO, PPO)</h3>
  <p>
      RL algorithms have produced some of the most impressive results in reasoning:
  </p>

  <p>
      <strong>PPO (Proximal Policy Optimization)</strong> uses a clipped surrogate objective to prevent too large policy updates, ensuring stable training. It typically employs a value function to estimate expected returns and compute advantages for action selection.
  </p>

  <h3 id="dpo">Direct Preference Optimization (DPO)</h3>
  <p>
      DPO and its variants offer alternatives to traditional RL:
  </p>

  <p>
      <strong>Standard DPO</strong> directly optimizes policy parameters based on preference data, without a separate reward model. It simplifies the training process while achieving comparable or better performance than PPO-based methods in many scenarios.
  </p>

  <h3 id="training-verifiers">Training verifiers and reward models</h3>
  <p>
      Effective verifiers are crucial for both inference scaling and learning-to-reason:
  </p>

  <p>
      <strong>Outcome Reward Models (ORMs)</strong> evaluate only the final answer, providing a simpler but less informative signal for optimization. They can be trained with binary classification (correct/incorrect) or pairwise preference data.
  </p>

  <h2 id="domain-specific" style="font-weight: bold; margin-top: -80px; padding-top: 80px;">Domain-Specific Reasoning Applications</h2>
  <hr>

  <h3 id="mathematical">Mathematical reasoning</h3>
  <p>
      Mathematical reasoning has served as a critical testbed for LLM reasoning capabilities:
  </p>

  <p>
      <strong>Informal Approaches</strong> treat math problems as natural language tasks, fine-tuning LLMs on carefully curated problem-solving datasets. Systems like DeepSeekMath and MetaMath have demonstrated impressive capabilities by combining mathematical text training with tree-based search and tool integration.
  </p>

  <h3 id="code-generation">Code generation</h3>
  <p>
      Code generation represents another domain where reasoning is especially valuable:
  </p>

  <p>
      <strong>Function-Level Completion</strong> has evolved into more complex <strong>Competition-Level Coding</strong>, with models tackling problems requiring multi-step reasoning and algorithmic thinking. Recent advances like DeepSeek-R1 and OpenAI's o3 have shown that end-to-end reinforcement learning can produce highly capable coding models.
  </p>

  <h3 id="tabular-data">Tabular data reasoning</h3>
  <p>
      Reasoning over structured data presents unique challenges:
  </p>

  <p>
      <strong>Table Transformation</strong> methods convert tabular data into formats that LLMs can process effectively. Techniques include serialization, specialized prompting, and embedding methods to preserve structural information.
  </p>

  <h3 id="game-theoretic">Game-theoretic reasoning</h3>
  <p>
      Strategic social reasoning in game-theoretic scenarios requires sophisticated cognitive abilities:
  </p>

  <p>
      <strong>Theory-of-Mind Modeling</strong> enables LLMs to attribute mental states to themselves and others, improving understanding and reasoning in social scenarios. Recent studies show that LLMs exhibit ToM capabilities that can be leveraged for strategic reasoning.
  </p>

  <h2 id="key-trends" style="font-weight: bold; margin-top: -80px; padding-top: 80px;">Key Trends and Breakthroughs</h2>
  <hr>

  <h3 id="shift">The shift from inference scaling to learning</h3>
  <p>
      A clear trend in LLM reasoning is the transition from inference-time techniques to learning-based approaches:
  </p>

  <p>
      <strong>Early Success of Inference Scaling</strong> like Chain-of-Thought prompting demonstrated that LLMs could reason more effectively with additional test-time computation. These methods proved valuable for enhancing performance without the cost of retraining.
  </p>

  <h3 id="agentic-systems">The emergence of agentic reasoning systems</h3>
  <p>
      Another significant trend is the evolution from standalone models to agentic systems:
  </p>

  <p>
      <strong>Single-Agent Systems</strong> enhance LLMs with tool use, retrieval, and environmental interaction, dramatically expanding their capabilities. Recent systems like OpenAI's Deep Research demonstrate how agents can leverage the web and specialized tools to solve complex problems beyond the reach of standalone models.
  </p>

  <h3 id="milestones">Recent model milestones (DeepSeek-R1, OpenAI o1)</h3>
  <p>
      Recent breakthroughs highlight the rapid progress in LLM reasoning:
  </p>

  <p>
      <strong>OpenAI's o1</strong> demonstrated the effectiveness of inference-time scaling in domains like mathematics, coding, and scientific problem-solving. It showed how structured thinking processes can dramatically improve performance on complex tasks requiring multi-step reasoning.
  </p>

  <h2 id="open-challenges" style="font-weight: bold; margin-top: -80px; padding-top: 80px;">Open Challenges and Future Directions</h2>
  <hr>

  <h3 id="evaluating">Evaluating reasoning beyond outcomes</h3>
  <p>
      Current evaluation methods often focus solely on whether models arrive at correct answers, neglecting the quality of the reasoning process:
  </p>

  <p>
      <strong>Process Evaluation</strong> remains challenging, as it's difficult even for humans to assess reasoning quality consistently. Recent approaches use metrics for coherence, redundancy, factuality, and contextual faithfulness, but more comprehensive frameworks are needed.
  </p>

  <h3 id="understanding-mechanisms">Understanding reasoning mechanisms</h3>
  <p>
      Despite impressive empirical results, our theoretical understanding of how LLMs reason remains limited:
  </p>

  <p>
      <strong>Empirical Analysis</strong> reveals that LLMs show strong performance on arithmetic, logical, and algorithmic reasoning but struggle with non-symbolic tasks and tool-augmented reasoning. Studies of model activations suggest different layers specialize in different aspects of reasoning.
  </p>

  <h3 id="data-challenges">Data challenges in scaling reasoning capabilities</h3>
  <p>
      The data requirements for training reasoning models present significant challenges:
  </p>

  <p>
      <strong>Limited High-Quality Data</strong> for reasoning tasks makes it difficult to scale supervised learning approaches. This is particularly true for competition-level problems and specialized domains where expert annotations are scarce.
  </p>

  <h3 id="cost-aware">Cost-aware training approaches</h3>
  <p>
      As models and training procedures grow more complex, efficiency becomes increasingly important:
  </p>

  <p>
      <strong>Dynamic Resource Allocation</strong> methods predict the difficulty of queries and allocate computation accordingly, optimizing the tradeoff between cost and performance. This can make inference more efficient by focusing resources where they're most needed.
  </p>

  <h2 id="conclusion" style="font-weight: bold; margin-top: -80px; padding-top: 80px;">Conclusion</h2>
  <hr>

  <h3 id="future">The future of reasoning in LLMs</h3>
  <p>
      The evolution of LLM reasoning capabilities represents one of the most exciting frontiers in AI research:
  </p>

  <p>
      <strong>Integrated Approaches</strong> that combine the strengths of different regimes and architectures are likely to dominate future developments. Models that learn to reason effectively while leveraging strategic inference-time techniques offer a promising path forward.
  </p>

  <h3 id="implications">Implications for AI development</h3>
  <p>
      The advancement of reasoning capabilities in LLMs has profound implications for AI development:
  </p>

  <p>
      <strong>Path to More Capable AI</strong> systems that can tackle increasingly complex problems requiring multi-step thinking and strategic planning. Reasoning capabilities may prove essential for progress toward more general artificial intelligence.
  </p>

  <p>
      As research continues to advance along both the regime and architecture dimensions, we can expect LLM reasoning to become increasingly sophisticated, reliable, and useful across a growing range of applications. The journey from inference to learning represents not just a technical evolution but a fundamental shift in how we approach the development of intelligent systems—one that may ultimately lead to AI systems capable of the kind of deliberate, step-by-step thinking that has long been considered a hallmark of human intelligence.
  </p>

</div>



  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
